{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b35385a-43ff-48b2-a121-50f64339bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "df_base = pd.read_csv('./data/mtsamples.csv')\n",
    "df_base['transcription'] = df_base.transcription.astype(str)\n",
    "df_base['description'] = df_base.description.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93963087-cf22-453d-9f1e-640c3cdd303a",
   "metadata": {},
   "source": [
    "# BioBART v2 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16bd936-c898-44a1-9ed2-36de4c897679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779fb48",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfa4b8-1066-489f-bc2c-6619735b5b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"GanjinZero/biobart-v2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    use_fast = True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    device_map = \"auto\", \n",
    "    torch_dtype = \"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a61ae",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc1af8-5622-4604-91e4-5ea74bfb78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_biobart(medical_text):\n",
    "    try:\n",
    "        prompt = f\"{medical_text.strip()}\"\n",
    "        inputs = tokenizer(\n",
    "            prompt, \n",
    "            return_tensors = \"pt\", \n",
    "            max_length = 1024, \n",
    "            truncation = True, \n",
    "            padding = True\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(**inputs,\n",
    "                max_new_tokens = 40, \n",
    "                num_beams = 4, \n",
    "                length_penalty = 1.5, \n",
    "                early_stopping = False, \n",
    "                no_repeat_ngram_size = 3, \n",
    "                encoder_no_repeat_ngram_size = 3\n",
    "              )\n",
    "\n",
    "        return tokenizer.decode(generated[0], skip_special_tokens = True)\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7454f4c-89d2-42ec-b023-6b94d05840dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_base.copy(deep = True)\n",
    "df['model-summary'] = df.transcription.apply(summarize_biobart)\n",
    "df['model-name'] = 'biobart'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d31ecd-7a26-4e6b-96d5-985b9fbea139",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c00386-2aa3-4ef1-9e8a-184a788a58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/mtsamples_with_biobart.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f800e6-89e1-4aeb-bea5-7910ca30a11f",
   "metadata": {},
   "source": [
    "# Med-Gemma 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175c716-7a6b-4b56-8aa7-80a979f60a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d12d77-5381-467e-acc3-1c472e23dad6",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61b74a2-86a5-4d0e-a866-d211efbffd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    return_tensors = \"pt\",\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map = {\"\": 1},\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = torch.compile(model, mode = \"reduce-overhead\",\n",
    "                      fullgraph = False,\n",
    "                      dynamic = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7846d-f5be-40e3-a74b-513faddc7ded",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bac0f1-c02e-4da9-960a-3f9c4e675142",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens = 128,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.1,\n",
    "    do_sample = True,\n",
    "    no_repeat_ngram_size = 6,\n",
    ")    \n",
    "\n",
    "def summarize_medgemma(medical_text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(f\"\"\"\n",
    "                Below is an abstract from a medical paper.\n",
    "    \n",
    "                ```text\n",
    "                {medical_text.strip()}\n",
    "                ```\n",
    "    \n",
    "                **Task:** Produce a 20-word summary **and end with a full stop (.) when you are done.**\n",
    "                Use clear, professional medical language.\n",
    "                Don't include a greeting or introduction.\n",
    "            \"\"\"),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = True,\n",
    "        padding = True,\n",
    "        max_length = 1024,\n",
    "        truncation = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            encoded,  \n",
    "            generation_config = gen_cfg, \n",
    "            return_dict_in_generate = False,\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer.decode(generated[0], skip_special_tokens = True)\n",
    "    summary = summary.split('\\nmodel\\n')[-1]\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a269943-1b26-4c52-966e-9a2cdca451a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_base.copy(deep = True)\n",
    "df['model-summary'] = df.transcription.apply(summarize_medgemma)\n",
    "df['model-name'] = 'med-gemma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b3069-7ecd-43f2-929f-46ef2b694e2c",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d2516-d839-4d30-a178-bef713e979d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/mtsamples_with_gemma.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf2ab5-07e6-4165-83bf-13f9f8cdd5f5",
   "metadata": {},
   "source": [
    "# Med-Llama 3.8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5559117-b95f-4707-a701-ad8201bd0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a7889-6423-42e1-8f6e-aa918fdbd2d6",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e454c0-1ffe-4e27-b18b-ae2a783d642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"johnsnowlabs/JSL-MedLlama-3-8B-v2.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    return_tensors=\"pt\",\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, \n",
    "                                            torch_dtype = torch.float16,\n",
    "                                            device_map = {\"\": 1},\n",
    "                                            trust_remote_code = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "model = torch.compile(model, mode = \"reduce-overhead\",\n",
    "                      fullgraph = False,\n",
    "                      dynamic = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e51df-31ac-45a4-9541-ca6b8bcbf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens = 64,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.1,\n",
    "    do_sample = True,\n",
    "    no_repeat_ngram_size = 6,\n",
    ")\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = textwrap.dedent(\"\"\"\n",
    "    <|im_start|>system\n",
    "    You are a concise, professional medical writing assistant. <|im_end|>\n",
    "    {% for m in messages %}\n",
    "    <|im_start|>{{ m['role'] }}\n",
    "    {{ m['content'] }}<|im_end|>\n",
    "    {% endfor %}\n",
    "    {% if add_generation_prompt %}<|im_start|>assistant\n",
    "    {% endif %}\n",
    "    \"\"\").strip()\n",
    "\n",
    "def summarize_medllama(medical_text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(f\"\"\"\n",
    "                Below is an abstract from a medical paper.\n",
    "    \n",
    "                ```text\n",
    "                {medical_text.strip()}\n",
    "                ```\n",
    "    \n",
    "                **Task:** Produce a 20-word summary **and end with a full stop (.) when you are done.**\n",
    "                Use clear, professional medical language.\n",
    "                Don't include a greeting or introduction.\n",
    "            \"\"\"),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = False\n",
    "    )\n",
    "\n",
    "    encoded = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **encoded,\n",
    "            generation_config = gen_cfg, \n",
    "            return_dict_in_generate = False, \n",
    "            max_new_tokens = 64,\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer.decode(generated[0], skip_special_tokens = True)\n",
    "    response_text = textwrap.fill(summary, 90).split('im_start|>assistant')[-1]\n",
    "    response_text = response_text.replace('<|im_end|>', '').replace(\"\\n\", \" \").strip()\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730687af-9551-4c6d-ab05-089bf1047c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_base.copy(deep = True)\n",
    "df['model-summary'] = df.transcription.apply(summarize_medllama)\n",
    "df['model-name'] = 'med-llama'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee97e9-48d9-4f65-8a2f-0307a8eaf734",
   "metadata": {},
   "source": [
    "## Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65638d1f-2ae5-4d3a-9778-fb12ffa1fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/mtsamples_with_llama.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
