{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469430e9-1a9a-4273-9f08-7d29d9d1382c",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b35385a-43ff-48b2-a121-50f64339bc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "df_base = pd.read_csv('./data/mtsamples.csv')\n",
    "df_base['transcription'] = df_base.transcription.astype(str)\n",
    "df_base['description'] = df_base.description.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93963087-cf22-453d-9f1e-640c3cdd303a",
   "metadata": {},
   "source": [
    "---\n",
    "## BioBART v2 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16bd936-c898-44a1-9ed2-36de4c897679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kilnaar/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-07 10:18:38.532350: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-07 10:18:38.563076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-07 10:18:38.563101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-07 10:18:38.563943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-07 10:18:38.569309: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-07 10:18:39.170035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779fb48",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dcfa4b8-1066-489f-bc2c-6619735b5b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(85401, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(85401, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(85401, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=85401, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ID = \"GanjinZero/biobart-v2-large\"\n",
    "tokenizer_biobart = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    use_fast = True)\n",
    "model_biobart = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    device_map = \"auto\", \n",
    "    torch_dtype = \"auto\")\n",
    "model_biobart.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a61ae",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cc1af8-5622-4604-91e4-5ea74bfb78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_biobart(medical_text):\n",
    "    try:\n",
    "        prompt = f\"{medical_text.strip()}\"\n",
    "        inputs = tokenizer_biobart(\n",
    "            prompt, \n",
    "            return_tensors = \"pt\", \n",
    "            max_length = 1024, \n",
    "            truncation = True, \n",
    "            padding = True\n",
    "        ).to(model_biobart.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model_biobart.generate(**inputs,\n",
    "                max_new_tokens = 40, \n",
    "                num_beams = 4, \n",
    "                length_penalty = 1.5, \n",
    "                early_stopping = False, \n",
    "                no_repeat_ngram_size = 3, \n",
    "                encoder_no_repeat_ngram_size = 3\n",
    "              )\n",
    "\n",
    "        return tokenizer_biobart.decode(generated[0], skip_special_tokens = True)\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7454f4c-89d2-42ec-b023-6b94d05840dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_base.copy(deep = True)\n",
    "df['model-summary'] = df.transcription.apply(summarize_biobart)\n",
    "df['model-name'] = 'biobart'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d31ecd-7a26-4e6b-96d5-985b9fbea139",
   "metadata": {},
   "source": [
    "### Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c00386-2aa3-4ef1-9e8a-184a788a58f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/mtsamples_with_biobart.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f800e6-89e1-4aeb-bea5-7910ca30a11f",
   "metadata": {},
   "source": [
    "---\n",
    "## Med-Gemma 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8175c716-7a6b-4b56-8aa7-80a979f60a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d12d77-5381-467e-acc3-1c472e23dad6",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61b74a2-86a5-4d0e-a866-d211efbffd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Gemma3ForConditionalGeneration(\n",
       "    (model): Gemma3Model(\n",
       "      (vision_tower): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(4096, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (self_attn): SiglipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      )\n",
       "      (language_model): Gemma3TextModel(\n",
       "        (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-33): 34 x Gemma3DecoderLayer(\n",
       "            (self_attn): Gemma3Attention(\n",
       "              (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "              (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Gemma3MLP(\n",
       "              (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "              (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "              (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): Gemma3RotaryEmbedding()\n",
       "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "tokenizer_gemma = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    return_tensors = \"pt\",\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "model_gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map = {\"\": 1},\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True\n",
    ")\n",
    "model_gemma.config.pad_token_id = tokenizer_gemma.pad_token_id\n",
    "model_gemma = torch.compile(model_gemma, mode = \"reduce-overhead\",\n",
    "                      fullgraph = False,\n",
    "                      dynamic = True)\n",
    "model_gemma.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7846d-f5be-40e3-a74b-513faddc7ded",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5bac0f1-c02e-4da9-960a-3f9c4e675142",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens = 128,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.1,\n",
    "    do_sample = True,\n",
    "    no_repeat_ngram_size = 6,\n",
    ")    \n",
    "\n",
    "def summarize_medgemma(medical_text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(f\"\"\"\n",
    "                Below is an abstract from a medical paper.\n",
    "    \n",
    "                ```text\n",
    "                {medical_text.strip()}\n",
    "                ```\n",
    "    \n",
    "                **Task:** Produce a 20-word summary **and end with a full stop (.) when you are done.**\n",
    "                Use clear, professional medical language.\n",
    "                Don't include a greeting or introduction.\n",
    "            \"\"\"),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    encoded = tokenizer_gemma.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = True,\n",
    "        padding = True,\n",
    "        max_length = 1024,\n",
    "        truncation = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(model_gemma.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model_gemma.generate(\n",
    "            encoded,  \n",
    "            generation_config = gen_cfg, \n",
    "            return_dict_in_generate = False,\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer_gemma.decode(generated[0], skip_special_tokens = True)\n",
    "    summary = summary.split('\\nmodel\\n')[-1]\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a269943-1b26-4c52-966e-9a2cdca451a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'pad_token_id': 0, 'bos_token_id': 2, 'eos_token_id': [1, 106]}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "df = df_base.copy(deep = True)\n",
    "df['model-summary'] = df.transcription.apply(summarize_medgemma)\n",
    "df['model-name'] = 'med-gemma'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b3069-7ecd-43f2-929f-46ef2b694e2c",
   "metadata": {},
   "source": [
    "### Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "573d2516-d839-4d30-a178-bef713e979d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/mtsamples_with_gemma.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf2ab5-07e6-4165-83bf-13f9f8cdd5f5",
   "metadata": {},
   "source": [
    "---\n",
    "## Med-Llama 3.8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5559117-b95f-4707-a701-ad8201bd0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a7889-6423-42e1-8f6e-aa918fdbd2d6",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93e454c0-1ffe-4e27-b18b-ae2a783d642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(128256, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_ID = \"johnsnowlabs/JSL-MedLlama-3-8B-v2.0\"\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    return_tensors=\"pt\",\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(MODEL_ID, \n",
    "                                            torch_dtype = torch.float16,\n",
    "                                            device_map = {\"\": 0},\n",
    "                                            trust_remote_code = True)\n",
    "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
    "model_llama.config.pad_token_id = tokenizer_llama.pad_token_id\n",
    "model_llama.generation_config.pad_token_id = tokenizer_llama.pad_token_id\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "model_llama = torch.compile(model_llama, mode = \"reduce-overhead\",\n",
    "                      fullgraph = False,\n",
    "                      dynamic = True)\n",
    "model_llama.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af1e51df-31ac-45a4-9541-ca6b8bcbf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens = 64,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.1,\n",
    "    do_sample = True,\n",
    "    no_repeat_ngram_size = 6,\n",
    ")\n",
    "\n",
    "if tokenizer_llama.chat_template is None:\n",
    "    tokenizer_llama.chat_template = textwrap.dedent(\"\"\"\n",
    "    <|im_start|>system\n",
    "    You are a concise, professional medical writing assistant. <|im_end|>\n",
    "    {% for m in messages %}\n",
    "    <|im_start|>{{ m['role'] }}\n",
    "    {{ m['content'] }}<|im_end|>\n",
    "    {% endfor %}\n",
    "    {% if add_generation_prompt %}<|im_start|>assistant\n",
    "    {% endif %}\n",
    "    \"\"\").strip()\n",
    "\n",
    "def summarize_medllama(medical_text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(f\"\"\"\n",
    "                Below is an abstract from a medical paper.\n",
    "    \n",
    "                ```text\n",
    "                {medical_text.strip()}\n",
    "                ```\n",
    "    \n",
    "                **Task:** Produce a 20-word summary **and end with a full stop (.) when you are done.**\n",
    "                Use clear, professional medical language.\n",
    "                Don't include a greeting or introduction.\n",
    "            \"\"\"),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer_llama.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = False\n",
    "    )\n",
    "\n",
    "    encoded = tokenizer_llama(\n",
    "        prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True).to(model_llama.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model_llama.generate(\n",
    "            **encoded,\n",
    "            generation_config = gen_cfg, \n",
    "            return_dict_in_generate = False, \n",
    "            max_new_tokens = 64,\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer_llama.decode(generated[0], skip_special_tokens = True)\n",
    "    response_text = textwrap.fill(summary, 90).split('im_start|>assistant')[-1]\n",
    "    response_text = response_text.replace('<|im_end|>', '').replace(\"\\n\", \" \").strip()\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "730687af-9551-4c6d-ab05-089bf1047c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'use_cache': False, 'pad_token_id': 128001, 'bos_token_id': 128000, 'eos_token_id': 128001}. If this is not desired, please set these values explicitly.\n"
     ]
    }
   ],
   "source": [
    "df = df_base.copy(deep = True)\n",
    "df['model-summary'] = df.transcription.apply(summarize_medllama)\n",
    "df['model-name'] = 'med-llama'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ee97e9-48d9-4f65-8a2f-0307a8eaf734",
   "metadata": {},
   "source": [
    "### Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65638d1f-2ae5-4d3a-9778-fb12ffa1fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/mtsamples_with_llama.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
