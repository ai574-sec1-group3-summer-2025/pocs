{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "709fc424-02af-466b-a728-0dd4067dfbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317d146-d2d6-4ecd-a3ca-66448ca2609a",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b23adc",
   "metadata": {},
   "source": [
    "## ROUGE F1 Scores\n",
    "\n",
    "An older-fashioned statistical set of metrics for similarity, typically used for summarizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81f840d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_calcer = rouge_scorer.RougeScorer([\n",
    "    'rougeL'\n",
    "], use_stemmer = True)\n",
    "\n",
    "def calc_rouge(short_text, full_text):\n",
    "    try:\n",
    "        rouges = rouge_calcer.score(short_text, full_text)\n",
    "        return rouges['rougeL'][2]\n",
    "    except Exception as e:\n",
    "        return 0.0\n",
    "\n",
    "def calc_rouge_pair(row):\n",
    "    return calc_rouge(row['description'], row['transcription']), calc_rouge(row[f'model-summary'], row['transcription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f1139a",
   "metadata": {},
   "source": [
    "## BERTScore\n",
    "\n",
    "Uses embeddings from the BERT transformer model to judge similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6182766",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def calc_bertscore(short_text, full_text):\n",
    "    try:\n",
    "        result = bertscore.compute(\n",
    "                predictions = [short_text],\n",
    "                references = [full_text],\n",
    "                model_type = \"microsoft/deberta-large-mnli\",\n",
    "                lang = \"en\",\n",
    "                device = \"cuda:1\"\n",
    "        )\n",
    "\n",
    "        return result[\"f1\"][0]\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def calc_bertscore_pair(row):\n",
    "    return pd.Series([calc_bertscore(row['description'], row['transcription']), calc_bertscore(row[f'model-summary'], row['transcription'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b2599",
   "metadata": {},
   "source": [
    "## BLEURT\n",
    "\n",
    "A regression model based on BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a72664ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /mnt/data/hf/metrics/bleurt/bleurt-tiny-128/downloads/extracted/599cd3ff6a3bbad54e145d867ccea405bb98c2b832fb29b50fb02089a1026530/bleurt-tiny-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /mnt/data/hf/metrics/bleurt/bleurt-tiny-128/downloads/extracted/599cd3ff6a3bbad54e145d867ccea405bb98c2b832fb29b50fb02089a1026530/bleurt-tiny-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "bleurt = evaluate.load(\"bleurt\", config_name = \"bleurt-tiny-128\")\n",
    "\n",
    "def calc_bleu(short_text, full_text):\n",
    "    # this was slower on GPU, I think it doesn't paralleize well because of how small the model is\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        try:\n",
    "            result = bleurt.compute(\n",
    "                    predictions = [short_text],\n",
    "                    references = [full_text],\n",
    "                    )\n",
    "            return result[\"scores\"][0]\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "def calc_bleu_pair(row):\n",
    "    return pd.Series([calc_bleu(row['description'], row['transcription']), calc_bleu(row[f'model-summary'], row['transcription'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc12339-0ead-4d4d-b529-bf074b6ca203",
   "metadata": {},
   "source": [
    "## HHEM 2.1 model from Vectara (WIP)\n",
    "\n",
    "Languaged-model-based system for detecting hallucinations in other LM operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d6eb47e-1abb-406c-92b6-35c6d62bbbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact hallucination_hhem_scorer:v0, 421.31MB. 8 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   8 of 8 files downloaded.  \n",
      "Done. 0:0:0.9 (461.7MB/s)\n",
      "Device set to use cuda:1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\"\n",
    "from weave.scorers import WeaveHallucinationScorerV1\n",
    "\n",
    "hallucination_scorer = WeaveHallucinationScorerV1(device = 'cuda:1')\n",
    "\n",
    "def hallucination_score(query, context, output):\n",
    "    try:\n",
    "        result = hallucination_scorer.score(\n",
    "            query = query,\n",
    "            context = context,\n",
    "            output = output,            \n",
    "        )\n",
    "    \n",
    "        return result.metadata['score']\n",
    "    except:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713af02-7125-4ccc-b7a4-9870074add55",
   "metadata": {},
   "source": [
    "## Coherence\n",
    "\n",
    "A fine-tuned deberta-small-long-nli Small Language Model that ensures the writing doesn't contradict itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f6864fe-f1d0-4d09-9ea4-46518aa94a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact coherence_scorer:v0, 549.59MB. 21 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   21 of 21 files downloaded.  \n",
      "Done. 0:0:1.2 (448.2MB/s)\n",
      "Device set to use cuda:1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"WANDB_BASE_URL\"] = \"https://api.wandb.ai\"\n",
    "from weave.scorers import WeaveCoherenceScorerV1\n",
    "\n",
    "coherence_scorer = WeaveCoherenceScorerV1(device = 'cuda:1')\n",
    "\n",
    "def coherence_score(query, output):\n",
    "    try:\n",
    "        result = coherence_scorer.score(\n",
    "            query = query,\n",
    "            output = output\n",
    "        )\n",
    "    \n",
    "        return result.metadata['score']\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978f851-4df4-4b48-8d0f-bf44117ad662",
   "metadata": {},
   "source": [
    "## Calculate and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2ed0a42-0b09-41f2-914d-67d672689bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'biobart',\n",
    "    'gemma',\n",
    "    'llama',    \n",
    "]\n",
    "\n",
    "for test_model_name in model_names:\n",
    "    df = pd.read_csv(f'./data/mtsamples_with_{test_model_name}.csv')\n",
    "    df['transcription'] = df.transcription.astype(str)\n",
    "\n",
    "    df[['rougeL_f1_source', 'rougeL_f1_dest']] = df.apply(calc_rouge_pair, axis = 1, result_type = 'expand')\n",
    "    df[['bertscore_f1_source', 'bertscore_f1_dest']] = df.apply(calc_bertscore_pair, axis = 1)\n",
    "    df[['bleurt_source', 'bleurt_dest']] = df.apply(calc_bleu_pair, axis = 1)\n",
    "    df['hallucination-score'] = df.apply(lambda x: hallucination_score(x.transcription, x.description, x['model-summary']), axis = 1)\n",
    "    df['coherence-score_source'] = df.apply(lambda x: coherence_score(x.transcription, x['description']), axis = 1)\n",
    "    df['coherence-score_dest'] = df.apply(lambda x: coherence_score(x.transcription, x['model-summary']), axis = 1)\n",
    "\n",
    "    # Save this model's stats to disk\n",
    "    df.to_csv(f'./data/mtsamples_with_{test_model_name}_model_scores.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e36d60-bd08-4ad0-80f3-6798e2cec5b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98794ac-0c2f-49ab-8963-6e66c11dc098",
   "metadata": {},
   "source": [
    "## Load the metrics for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af01cd-0902-412f-8a45-512c34d63a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/mtsamples_with_biobart_model_scores.csv')\n",
    "df = pd.concat([df, pd.read_csv('./data/mtsamples_with_gemma_model_scores.csv')], ignore_index = True)\n",
    "df = pd.concat([df, pd.read_csv('./data/mtsamples_with_llama_model_scores.csv')], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6be2d-2a91-47e0-a22a-21d074a0ecb5",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe21d66-f1f3-4263-959e-ed8ef180cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13368d31-5129-4ecf-abc4-55b3e763578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('model-name').count().iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ad687-d906-4149-9668-7b9603f06d26",
   "metadata": {},
   "source": [
    "# Metrics for the human-entered summaries\n",
    "\n",
    "the source values are the same for all models, so just pick one and get the averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4741bd-67c1-4901-9b46-8d127f4de25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_human =  df[df['model-name'] == 'biobart'][['rougeL_f1_source', 'bertscore_f1_source', 'bleurt_source', 'coherence-score_source']].mean()\n",
    "df_metrics_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6e4b7-9d5b-420c-bb6f-7665ab921910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the machine-generated summaries for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70ec17-8b48-452b-877d-17ab55fe868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_models = df.groupby('model-name')[['rougeL_f1_dest', 'bertscore_f1_dest', 'bleurt_dest', 'hallucination-score', 'coherence-score_dest']].mean()\n",
    "df_metrics_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b259d-b709-4d0a-be18-c79072b7274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of machine-generated and human-generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe434c38-0d6b-4957-8a99-22923b479bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "        df_metrics_models.rougeL_f1_dest - df_metrics_human.rougeL_f1_source, \n",
    "        df_metrics_models.bertscore_f1_dest - df_metrics_human.bertscore_f1_source,\n",
    "        df_metrics_models.bleurt_dest - df_metrics_human.bleurt_source,\n",
    "        df_metrics_models['coherence-score_dest'] - df_metrics_human['coherence-score_source'],\n",
    "    \n",
    "    ], axis = 1).rename(columns = {'rougeL_f1_dest': 'rougeL_f1_diff', \n",
    "                                   'bertscore_f1_dest': 'bertscore_f1_diff', \n",
    "                                   'bleurt_dest': 'bleurt_diff', \n",
    "                                   'coherence-score_dest': 'coherence-score_diff'})\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
