{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469430e9-1a9a-4273-9f08-7d29d9d1382c",
   "metadata": {},
   "source": [
    "# Summarization Med-Llama 3.8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5559117-b95f-4707-a701-ad8201bd0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a7889-6423-42e1-8f6e-aa918fdbd2d6",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e454c0-1ffe-4e27-b18b-ae2a783d642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"johnsnowlabs/JSL-MedLlama-3-8B-v2.0\"\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    return_tensors=\"pt\",\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "model_llama = AutoModelForCausalLM.from_pretrained(MODEL_ID, \n",
    "                                            torch_dtype = torch.float16,\n",
    "                                            device_map = {\"\": 0},\n",
    "                                            trust_remote_code = True)\n",
    "tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
    "model_llama.config.pad_token_id = tokenizer_llama.pad_token_id\n",
    "model_llama.generation_config.pad_token_id = tokenizer_llama.pad_token_id\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "model_llama = torch.compile(model_llama, mode = \"reduce-overhead\",\n",
    "                      fullgraph = False,\n",
    "                      dynamic = True)\n",
    "model_llama.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e51df-31ac-45a4-9541-ca6b8bcbf8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens = 64,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.1,\n",
    "    do_sample = True,\n",
    "    no_repeat_ngram_size = 6,\n",
    ")\n",
    "\n",
    "if tokenizer_llama.chat_template is None:\n",
    "    tokenizer_llama.chat_template = textwrap.dedent(\"\"\"\n",
    "    <|im_start|>system\n",
    "    You are a concise, professional medical writing assistant. <|im_end|>\n",
    "    {% for m in messages %}\n",
    "    <|im_start|>{{ m['role'] }}\n",
    "    {{ m['content'] }}<|im_end|>\n",
    "    {% endfor %}\n",
    "    {% if add_generation_prompt %}<|im_start|>assistant\n",
    "    {% endif %}\n",
    "    \"\"\").strip()\n",
    "\n",
    "def summarize_medllama(medical_text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(f\"\"\"\n",
    "                Below is an abstract from a medical paper.\n",
    "    \n",
    "                ```text\n",
    "                {medical_text.strip()}\n",
    "                ```\n",
    "    \n",
    "                **Task:** Produce a 20-word summary **and end with a full stop (.) when you are done.**\n",
    "                Use clear, professional medical language.\n",
    "                Don't include a greeting or introduction.\n",
    "            \"\"\"),\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer_llama.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = False\n",
    "    )\n",
    "\n",
    "    encoded = tokenizer_llama(\n",
    "        prompt,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True).to(model_llama.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated = model_llama.generate(\n",
    "            **encoded,\n",
    "            generation_config = gen_cfg, \n",
    "            return_dict_in_generate = False, \n",
    "            max_new_tokens = 64,\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer_llama.decode(generated[0], skip_special_tokens = True)\n",
    "    response_text = textwrap.fill(summary, 90).split('im_start|>assistant')[-1]\n",
    "    response_text = response_text.replace('<|im_end|>', '').replace(\"\\n\", \" \").strip()\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1cf402-f913-454d-92c3-75eda47eb260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A 25-year-old white female presents with migraines despite trying various medications and experiencing recurrent sinus infections..'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "This 25 year old white female.  This young woman presents with complaint of migranes.\n",
    "She had trying many medications.\n",
    "She has frequent sinus infections.\n",
    "She has tried many drugs to help the migranes.\n",
    "\"\"\"\n",
    "summary = summarize_medllama(text)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca646cd-130b-42e0-bae6-643ec38c90e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 131)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text), len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db31aa4-003b-44cf-a86b-121c1915ae95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
