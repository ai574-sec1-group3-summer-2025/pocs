{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63812aa9-b420-47ff-9eb1-1ad6d6a46353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031840c-79c4-4ce4-9fbc-d2b2cf11c1ed",
   "metadata": {},
   "source": [
    "# Load model\n",
    "https://huggingface.co/luqh/ClinicalT5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a649315d-4259-4080-b6d6-b2ec974f9122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All Flax model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the Flax model and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m MODEL_ID = \u001b[33m\"\u001b[39m\u001b[33mluqh/ClinicalT5-large\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, model_max_length = \u001b[32m1024\u001b[39m)\n\u001b[32m      3\u001b[39m model = \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#low_cpu_mem_usage = False,\u001b[39;49;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/modeling_utils.py:3848\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3843\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3844\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3845\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3846\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3847\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1352\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/torch/nn/modules/module.py:1348\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1348\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1349\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1350\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhen moving module from meta to a different device.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1351\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1352\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1353\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device."
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"luqh/ClinicalT5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, model_max_length = 1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    from_flax = True, \n",
    "    low_cpu_mem_usage = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608af921-9868-49f9-a3e0-575cfafb532e",
   "metadata": {},
   "source": [
    "# Some medical text\n",
    "\n",
    "Not that this will fail if there are too many tokens. We'll have to do some chunking or RAG in order to\n",
    "deal with the small context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bdeb259-5f30-4dd2-8284-3a1099c00756",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_text = \"\"\"\n",
    "Non-steroidal anti-inflammatory drugs are not only potent analgesics and antipyretics but also nephrotoxins, and may cause \n",
    "electrolyte disarray. In addition to the commonly expected effects, including hyperkalemia, hyponatremia, acute renal injury, \n",
    "renal cortical necrosis, and volume retention, glomerular disease with or without nephrotic syndrome or nephritis can occur as \n",
    "well including after years of seemingly safe administration. Minimal change disease, secondary membranous glomerulonephritis, \n",
    "and acute interstitial nephritis are all reported glomerular lesions seen with non-steroidal anti-inflammatory use. We report a \n",
    "patient who used non-steroidal anti-inflammatory drugs for years without diabetes, chronic kidney disease, or proteinuria; he \n",
    "then developed severe nephrotic range proteinuria with 7â€‰g of daily urinary protein excretion. Renal biopsy showed minimal \n",
    "change nephropathy, a likely secondary membranous glomerulonephritis, and acute interstitial nephritis present simultaneously\n",
    "in one biopsy. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff1ba37d-4df4-457e-81f2-0c681be5f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(medical_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69835ce5-5ab0-45ab-9f92-2aa0e166cdef",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c10d823c-2b89-46f6-9f09-50cc2633fcca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor.item() cannot be called on meta tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m inputs = tokenizer(prompt, return_tensors = \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     summary_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_no_repeat_ngram_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/generation/utils.py:2378\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2375\u001b[39m batch_size = inputs_tensor.shape[\u001b[32m0\u001b[39m]\n\u001b[32m   2377\u001b[39m device = inputs_tensor.device\n\u001b[32m-> \u001b[39m\u001b[32m2378\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_has_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[32m   2381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder:\n\u001b[32m   2382\u001b[39m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[32m   2383\u001b[39m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/generation/utils.py:2158\u001b[39m, in \u001b[36mGenerationMixin._prepare_special_tokens\u001b[39m\u001b[34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[39m\n\u001b[32m   2153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m decoder_start_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2156\u001b[39m     )\n\u001b[32m   2157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m2158\u001b[39m     eos_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2159\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m isin_mps_friendly(elements=eos_token_tensor, test_elements=pad_token_tensor).any()\n\u001b[32m   2160\u001b[39m ):\n\u001b[32m   2161\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kwargs_has_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs_has_attention_mask:\n\u001b[32m   2162\u001b[39m         logger.warning_once(\n\u001b[32m   2163\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2164\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2165\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`attention_mask` to obtain reliable results.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2166\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/torch/_meta_registrations.py:7088\u001b[39m, in \u001b[36mmeta_local_scalar_dense\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   7086\u001b[39m \u001b[38;5;129m@register_meta\u001b[39m(aten._local_scalar_dense)\n\u001b[32m   7087\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeta_local_scalar_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Tensor):\n\u001b[32m-> \u001b[39m\u001b[32m7088\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor.item() cannot be called on meta tensors\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensor.item() cannot be called on meta tensors"
     ]
    }
   ],
   "source": [
    "prompt = f\"summarize: {medical_text.strip()}\"\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    summary_ids = model.generate(**inputs,\n",
    "        max_new_tokens = 128, \n",
    "        num_beams = 4, \n",
    "        length_penalty = 1.5, \n",
    "        early_stopping = False, \n",
    "        no_repeat_ngram_size = 3, \n",
    "        encoder_no_repeat_ngram_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9da19-242e-46c6-8bc4-9724a9a08409",
   "metadata": {},
   "outputs": [],
   "source": [
    "for summary_id in summary_ids:\n",
    "    summary = tokenizer.decode(summary_id, skip_special_tokens = True)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126039e-60b9-4093-b2d7-97403035fae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
