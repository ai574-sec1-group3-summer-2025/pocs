{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63812aa9-b420-47ff-9eb1-1ad6d6a46353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031840c-79c4-4ce4-9fbc-d2b2cf11c1ed",
   "metadata": {},
   "source": [
    "# Load model\n",
    "https://huggingface.co/luqh/ClinicalT5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a649315d-4259-4080-b6d6-b2ec974f9122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "All Flax model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the Flax model and are newly initialized: ['encoder.embed_tokens.weight', 'lm_head.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"luqh/ClinicalT5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, from_flax=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608af921-9868-49f9-a3e0-575cfafb532e",
   "metadata": {},
   "source": [
    "# Some medical text\n",
    "\n",
    "Not that this will fail if there are too many tokens. We'll have to do some chunking or RAG in order to\n",
    "deal with the small context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bdeb259-5f30-4dd2-8284-3a1099c00756",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_text = \"\"\"\n",
    "The umbilical skin was elevated with a towel clip and the Veress needle was inserted without difficulty.  Saline drop test proved entrance into the abdominal cavity and then the abdomen was insufflated to sufficient pressure of 15 mmHg.  Next, the Veress was removed and #10 bladed trocar was inserted without difficulty.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69835ce5-5ab0-45ab-9f92-2aa0e166cdef",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e925c8c0-0e7b-463e-bd1b-e416b31f40ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"summarize: \" + medical_text.strip()\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation = True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    summary_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=80,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2df5fcb5-43f4-4416-8743-4221034a90f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the umbilical skin was elevated with a towel clip and the veress needle was inserted without difficulty.\n"
     ]
    }
   ],
   "source": [
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
