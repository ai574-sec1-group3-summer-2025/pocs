{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63812aa9-b420-47ff-9eb1-1ad6d6a46353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031840c-79c4-4ce4-9fbc-d2b2cf11c1ed",
   "metadata": {},
   "source": [
    "# Load model\n",
    "https://huggingface.co/luqh/ClinicalT5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649315d-4259-4080-b6d6-b2ec974f9122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "All Flax model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the Flax model and are newly initialized: ['decoder.embed_tokens.weight', 'lm_head.weight', 'encoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"luqh/ClinicalT5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_ID, model_max_length = 1024)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_ID, from_flax = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608af921-9868-49f9-a3e0-575cfafb532e",
   "metadata": {},
   "source": [
    "# Some medical text\n",
    "\n",
    "Not that this will fail if there are too many tokens. We'll have to do some chunking or RAG in order to\n",
    "deal with the small context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bdeb259-5f30-4dd2-8284-3a1099c00756",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_text = \"\"\"\n",
    "After induction of general anesthesia, the patient was placed prone on the operating room table \n",
    "resting on chest rolls.  Her face was resting in a pink foam headrest.  Extreme care was taken positioning her because she \n",
    "weighs 92 kg.  There was a lot of extra padding for her limbs and her limbs were positioned comfortably.  The arms were not \n",
    "hyperextended.  Great care was taken with positioning of the head and making sure there was no pressure on her eyes especially \n",
    "since she already has visual disturbance.  A Foley catheter was in place.  She received IV Cipro 400 mg because she is \n",
    "allergic to most antibiotics.,Fluoroscopy was used to locate the lower end of the fractured catheter and the skin was marked.  \n",
    "It was also marked where we would try to insert the new catheter at the L4 or L3 interspinous space.,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff1ba37d-4df4-457e-81f2-0c681be5f359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(medical_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69835ce5-5ab0-45ab-9f92-2aa0e166cdef",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e925c8c0-0e7b-463e-bd1b-e416b31f40ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kilnaar/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/generation/utils.py:1192: UserWarning: Unfeasible length constraints: `min_length` (60) is larger than the maximum possible length (21). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length. Note that `max_length` is set to 21, its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what procedure was performed in: \" + medical_text.strip()\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\", truncation = False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    summary_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens = 20,\n",
    "        min_length = 60,\n",
    "        num_beams = 8,\n",
    "        length_penalty = 0.1,\n",
    "        early_stopping = False,      # let all beams finish\n",
    "        no_repeat_ngram_size = 2     # reduce verbatim copying\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df5fcb5-43f4-4416-8743-4221034a90f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fluoroscopy was used to locate the lower end of the fractured catheter.,\n"
     ]
    }
   ],
   "source": [
    "for summary_id in summary_ids:\n",
    "    summary = tokenizer.decode(summary_id, skip_special_tokens = True)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0fb8a-fba5-4569-b9b7-25ab1be91e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1cef5-b8f7-43f8-90d2-9f66833a60ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
