{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dcd4bcf-556c-4619-a124-1b30508525f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57dc2f5c-4498-421c-a8f8-2d7bd268121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Gemma3ForConditionalGeneration(\n",
       "    (model): Gemma3Model(\n",
       "      (vision_tower): SiglipVisionModel(\n",
       "        (vision_model): SiglipVisionTransformer(\n",
       "          (embeddings): SiglipVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(4096, 1152)\n",
       "          )\n",
       "          (encoder): SiglipEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x SiglipEncoderLayer(\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (self_attn): SiglipAttention(\n",
       "                  (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): SiglipMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "        (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "      )\n",
       "      (language_model): Gemma3TextModel(\n",
       "        (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-33): 34 x Gemma3DecoderLayer(\n",
       "            (self_attn): Gemma3Attention(\n",
       "              (q_proj): Linear(in_features=2560, out_features=2048, bias=False)\n",
       "              (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
       "              (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Gemma3MLP(\n",
       "              (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "              (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
       "              (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): Gemma3RotaryEmbedding()\n",
       "        (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    return_tensors = \"pt\",\n",
    "    padding = True,\n",
    "    truncation = True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map = {\"\": 1},\n",
    "    torch_dtype = \"auto\",\n",
    "    trust_remote_code = True\n",
    ")\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "#model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "import os\n",
    "os.environ['TORCH_LOGS'] = \"recompiles\"\n",
    "\n",
    "model = torch.compile(model, mode = \"reduce-overhead\",\n",
    "                      fullgraph = False,\n",
    "                      dynamic = True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6072732b-64fa-4900-ae89-1642bb76a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_text = \"\"\"\n",
    "Non-steroidal anti-inflammatory drugs are not only potent analgesics and antipyretics but also nephrotoxins, and may cause \n",
    "electrolyte disarray. In addition to the commonly expected effects, including hyperkalemia, hyponatremia, acute renal injury, \n",
    "renal cortical necrosis, and volume retention, glomerular disease with or without nephrotic syndrome or nephritis can occur as \n",
    "well including after years of seemingly safe administration. Minimal change disease, secondary membranous glomerulonephritis, \n",
    "and acute interstitial nephritis are all reported glomerular lesions seen with non-steroidal anti-inflammatory use. We report a \n",
    "patient who used non-steroidal anti-inflammatory drugs for years without diabetes, chronic kidney disease, or proteinuria; he \n",
    "then developed severe nephrotic range proteinuria with 7 g of daily urinary protein excretion. Renal biopsy showed minimal \n",
    "change nephropathy, a likely secondary membranous glomerulonephritis, and acute interstitial nephritis present simultaneously\n",
    "in one biopsy. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd440072-008a-4317-b8f4-9930fc1ecce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_cfg = GenerationConfig(\n",
    "    max_new_tokens = 128,\n",
    "    temperature = 0.1,\n",
    "    top_p = 0.9,\n",
    "    repetition_penalty = 1.1,\n",
    "    do_sample = True,\n",
    "    no_repeat_ngram_size = 6,\n",
    ")    \n",
    "\n",
    "def summarize(medical_text):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": textwrap.dedent(f\"\"\"\n",
    "                Below is an abstract from a medical paper.\n",
    "    \n",
    "                ```text\n",
    "                {medical_text.strip()}\n",
    "                ```\n",
    "    \n",
    "                **Task:** Produce a 20-word summary **and end with a full stop (.) when you are done.**\n",
    "                Use clear, professional medical language.\n",
    "                Don't include a greeting or introduction.\n",
    "            \"\"\"),\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    encoded = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        tokenize = True,\n",
    "        padding = True,\n",
    "        max_length = 1024,\n",
    "        truncation = True\n",
    "    )\n",
    "\n",
    "    batch = tokenizer.pad(\n",
    "        [{\"input_ids\": encoded}],\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            input_ids = batch['input_ids'],  \n",
    "            generation_config = gen_cfg, \n",
    "            return_dict_in_generate = False,\n",
    "            attention_mask  = batch[\"attention_mask\"])\n",
    "    \n",
    "    summary = tokenizer.decode(generated[0], skip_special_tokens = True)\n",
    "    summary = summary.split('\\nmodel\\n')[-1]\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd8225f3-c343-4f75-b47b-a2508e522fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "V0712 16:20:57.093000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/1] [__recompiles] Recompiling function forward in /home/kilnaar/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:1275\n",
      "V0712 16:20:57.093000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/1] [__recompiles]     triggered by the following guard failure(s):\n",
      "V0712 16:20:57.093000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/1] [__recompiles]     - 0/0: tensor 'attention_mask' size mismatch at index 3. expected 1151, actual 402\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NSAID use can trigger diverse glomerular diseases, including minimal change disease, membranous glomerulonephritis and acute interstitial nephritis, even in patients without pre-existing conditions.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(medical_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cdd921c-f57e-433b-8b2a-fc066eb34b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "df = pd.read_csv('./mtsamples.csv')\n",
    "df['transcription'] = df.transcription.astype(str)\n",
    "df['description'] = df.description.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa23b84-c6a3-4ba8-a654-7d061f834176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0712 16:21:20.517000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/2] [__recompiles] Recompiling function forward in /home/kilnaar/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:1275\n",
      "V0712 16:21:20.517000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/2] [__recompiles]     triggered by the following guard failure(s):\n",
      "V0712 16:21:20.517000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/2] [__recompiles]     - 0/1: ___check_obj_id(past_key_values.key_cache[0], 136810302758672)\n",
      "V0712 16:21:20.517000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/2] [__recompiles]     - 0/0: tensor 'attention_mask' size mismatch at index 3. expected 1151, actual 526\n",
      "V0712 16:21:29.067000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/3] [__recompiles] Recompiling function forward in /home/kilnaar/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:1275\n",
      "V0712 16:21:29.067000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/3] [__recompiles]     triggered by the following guard failure(s):\n",
      "V0712 16:21:29.067000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/3] [__recompiles]     - 0/2: ___check_obj_id(past_key_values.key_cache[0], 136810688768880)\n",
      "V0712 16:21:29.067000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/3] [__recompiles]     - 0/1: ___check_obj_id(past_key_values.key_cache[0], 136810302758672)\n",
      "V0712 16:21:29.067000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/3] [__recompiles]     - 0/0: tensor 'attention_mask' size mismatch at index 3. expected 1151, actual 738\n",
      "V0712 16:21:37.594000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/4] [__recompiles] Recompiling function forward in /home/kilnaar/anaconda3/envs/ai574-pocs/lib/python3.11/site-packages/transformers/models/gemma3/modeling_gemma3.py:1275\n",
      "V0712 16:21:37.594000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/4] [__recompiles]     triggered by the following guard failure(s):\n",
      "V0712 16:21:37.594000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/4] [__recompiles]     - 0/3: ___check_obj_id(past_key_values.key_cache[0], 136810471603952)\n",
      "V0712 16:21:37.594000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/4] [__recompiles]     - 0/2: ___check_obj_id(past_key_values.key_cache[0], 136810688768880)\n",
      "V0712 16:21:37.594000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/4] [__recompiles]     - 0/1: ___check_obj_id(past_key_values.key_cache[0], 136810302758672)\n",
      "V0712 16:21:37.594000 180773 site-packages/torch/_dynamo/guards.py:3006] [0/4] [__recompiles]     - 0/0: ___check_obj_id(past_key_values.key_cache[0], 136810962487344)\n"
     ]
    }
   ],
   "source": [
    "df['med-gemma-summary'] = df.transcription.apply(summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec3702cb-ea8e-460e-beae-7eb8cb3224f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('mtsamples_with_gemma.csv', index = False, quoting = csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5e345-399e-46eb-90d1-158a17d1243b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
